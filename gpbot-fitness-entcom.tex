\documentclass[preprint]{elsarticle}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx,epsfig}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{listings}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{multirow}
%\usepackage[boxed]{algorithm2e}l
%\usepackage{algpseudocode}

\providecommand{\SetAlgoLined}{\SetLine}
\providecommand{\DontPrintSemicolon}{\dontprintsemicolon}
%%%%

\usepackage{color}
\usepackage{alltt}
\usepackage{verbatim}
\usepackage{moreverb} 
\usepackage{url}
%\usepackage[utf8]{inputenc}
\usepackage{inputenc}
%\usepackage[spanish]{babel}
\usepackage{url}

\begin{document}

\begin{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   TITLE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Analyzing the Influence of the Fitness Function on Genetically Programmed Bots for a Real-Time Strategy Game}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   AUTHORS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{B. Lind and A. Nonimous}
%\author{A. Fern{\'a}ndez-Ares, A.M. Mora, P. Garc{\'i}a-S{\'a}nchez, P.A. Castillo, J.J. Merelo}
%\ead{\{antares, amorag, pgarcia, pedro, jmerelo\}@geneura.ugr.es}
%\address{Departamento de Arquitectura y TecnologÃ­a de Computadores.\\ ETSIIT - CITIC. University of Granada, Spain}
%\address{Department of Computer Architecture and Technology\\ ETSIIT, CITIC. University of Granada, Spain}

%\maketitle

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{abstract}
% una o dos frases dando una breve introduccion general al area
Real-Time Strategy games (RTSs) are one of the common testbeds for research in the Computational Intelligence field.
% dos o tres frases mas detallando el background
The automatic generation of autonomous agents (bots) to play this kind of games is also a very prolific research line. However, the applied techniques to do this, must deal with uncertainty and real-time planning in order to control the game agents.
% una frase muy directa indicando el problema general a resolver en este trabajo
This work presents a Genetic Programming (GP) algorithm to create the behavioural engine of bots able to play an RTS. Three different fitness functions are proposed and a detailed analysis of their performance has been conducted. The aim is to get a better deal with this uncertainty. 
% una frase muy directa indicando el resultado obtenido en este trabajo (el mas destacable)
The results show that GP is a suitable method to generate bots to play a `simple' RTS game, Planet Wars. Moreover, it provides a mechanism to generate rules that enable to define very different behaviours.
% un par de frases indicando lo que aporta este resultado al estado del arte
The paper analyses several aspects of the bots, in addition to their final performance on battles, such as the evolution of the decision trees themselves, or the influence on the results of noise/uncertainty always present in the context of games.
% una ultima frase poniendo el resultado en un ambito general
As a general conclusion a victory-based fitness, which prioritises the number of victories, contributes to generate better bots, on average, than other functions based on the numerical aspects of the battles, such as the number of resources gathered, or the number of units generated. 
\end{abstract}
% Antonio - he hecho una mejora de lo que proponía Pedro, atendiendo también a lo que sugería JJ. A ver si os gusta. ;)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   KEYWORDS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{keyword}
Real-time Strategy game \sep Genetic Programming \sep Autonomous agent \sep Bot \sep Fitness function \sep Uncertainty
\end{keyword}

\end{frontmatter}


%-------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------------
\section{Introduction}
\noindent 

Real-Time Strategy (RTS) games are a genre of videogames where the action takes place in a scenario (or arena) and the contenders must fight, controlling a set of units, in order to monopolise the resources and, eventually, to defeat the opponent. These games are not turn-based, so the players must take decisions in real-time. This kind of games has become an interesting testbed in Computational Intelligence research \cite{Lara2013review}, as they can be seen as a simplification of more complex scenarios, such as road traffic simulation or financial prediction. As in these environments, in addition to the required real-time planning, there is some uncertainty present in RTSs.

In this scope, the automatic generation of agents (also called {\em bots}) to play this kind of games is one of the most prominent areas. For example, Evolutionary Algorithms (EAs) have been previously used for the creation of bots/strategies in RTSs such as {\em Planet Wars} \cite{Genebot_CEC11,ExpGenebot_CIG2012,Garcia14Treedepth} or {\em StarCraft} \cite{bistrom_GP_StarCraft,Barriga2014:BuildingOrder_GA,Garcia15Starcraft}, among others. 

% Antonio - introduzco aquí lo que son los EAs, porque viene bien para justificar la importancia del fitness. ;D
EAs \cite{EAs_Back96} are a class of probabilistic search and optimisation algorithms inspired on the model of Darwinistic evolution. 
There are several subtypes, depending on the data structure that is preferentially used for representing solutions, but Genetic Algorithms (GAs) \cite{GAs_Goldberg89} and Genetic Programming (GP) \cite{GP_Koza92} are the most extended. The main features are common to all of them: a population of individuals or potential solutions of the target problem, a selection method that favours better solutions, and a set of operators that act upon the
selected solutions. Thus, after an initial population is created (usually random), the selection and operators - crossover and mutation - are applied to the individuals, and the new population then replaces the older one. This is repeated for a number of generations or until another stop condition is met. 
If there is defined a correct {\em fitness function}, which assigns a reliable value to every individual, this process guarantees that the average quality of the population increases with the number of generations.
Thus, deciding the best evaluation function is a key factor for the success of these algorithms.

The aforementioned works apply GAs \cite{Genebot_CEC11,ExpGenebot_CIG2012,Barriga2014:BuildingOrder_GA}, and GP \cite{bistrom_GP_StarCraft,Garcia14Treedepth,Garcia15Starcraft}. 
All these approaches have in common the consideration of an evaluation function which measures the quality of the behaviour of the bots being evolved. 
Different fitness functions have been used in the past, for example the {\em game score}, or the {\em number of victories} against the rivals during the evaluation. However, these functions have to deal with the uncertainty, also called {\em noise} \cite{merelo14:noisy,merelo15:uncertainty}, present due to the stochastic nature of the opponent bots or to the game itself (rules or maps) . 

In addition, the selection of a fitness function or another, may generate different behaviours for the generated bots. For example, an evolutionary process using a victory-based fitness function would create `aggressive' bots, while one function, that also takes into account the number of different structures created, may generate a more `strategic' bot. 

Therefore, this paper aims to choose the best type of fitness function for a Genetic Programming-based approach, which generates {\em Decision Trees} - or rule-based systems - (DTs), as Artificial Intelligence engines for bots in the scope of RTSs.
To this end, we compare the bots created using different types of fitness, studying both, the generated DTs, and the performance of the bots in battles against unknown enemies and maps. 
We have applied GP to generate the bots because it provides a good mechanism to create rules from scratch, giving the possibility to define very different behaviours. 

Three fitness functions have been tested in the paper:
\begin{itemize}
\item A {\em victory-based fitness}, that mainly considers the number of battles won in the evaluation. Turns are also considered if there is a draw.
\item A {\em slope-based fitness}: that measures the behaviour during all the game (not only at the end, as in the previous one), using the slope of the percentage of possessed units in a turn of the game.
\item An {\em area-based fitness}: which computes an integral as the percentage of units possessed by the bot divided by the number of turns the battle has taken. 
\end{itemize}

These approaches have been implemented for the game {\em Planet Wars}, as it is a `simple' RTS: with only one type of resources and units. 
The rest of the paper is structured as follows: after the background and the problem description (Section \ref{section:background}), the state of the art is presented in Section \ref{sec:soa}. Then, the description of the GP approach and the considered fitness functions are described respectively in Sections \ref{sec:agent} and \ref{sec:fitness_functions}. Section \ref{sec:experiments} explains the experimental setup, while Section \ref{sec:results} analyses the obtained results. Finally, the conclusions and future lines work are presented in Section \ref{sec:conclusion}.



%-----------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------
\section{Background and problem description}
\label{section:background}
\subsection{Genetic Programming}

Genetic Programming (GP) \cite{GP_Koza92} is a type of EA where the individuals are represented as hierarchical structures (typically trees), which size and shape is not defined a priori, as in other evolutionary methods, but they evolve along the generations. Thus, GP performs a structural optimisation, which requires the use of specific genetic operators, focused on the management (and improvement) of this kind of structure.
The flow of a GP algorithm is the same as any other EA: a population is created at random, each individual in the population is evaluated using a fitness function, the individuals that performed better in the evaluation process have a higher probability of being selected as parents for the new population, and a new population is created by applying crossover and mutation operators with a certain probability. This process is repeated until a predefined termination criterion is met.

% ----------------------------------------------------------------------

\subsection{Planet Wars Game}
In this paper we work with a %simplified
version of the game Galcon \cite{wiki:galcon} designed as an arena where programmed bots
can fight. This version was used for the Google AI Challenge 2010
(GAIC)\footnote{http://planetwars.aichallenge.org/}. 

 \begin{figure}[ht]
 \begin{center}
   \epsfig{file=./imags/naves.pdf,width=7cm}
 \end{center}
 \caption{Simulated screenshot of an early stage of a run in Planet Wars. White planets belong to the player (blue colour in the game), dark grey belong to the opponent (red in the game), and light grey planets belong to no player. The triangles are fleets, and the numbers (in planets and triangles) represent the ships. The planet size means growth rate of the amount of ships in it (the bigger, the higher).}
 \label{figura:PlanetWars1}
 \end{figure}

A Planet Wars match takes place on a map (see Fig. \ref{figura:PlanetWars1})
that contains several planets (neutral, enemies, or own), each one of
them with a number assigned to it. This number represents the amount of ships
that the planet is currently hosting. 

The aim of the game is to destroy all the ships in planets owned by
the opponent. Although Planet Wars is a RTS game, this implementation
has transformed it into a turn-based game, in which each player has a
maximum number of turns to accomplish the objective. At the end of the
match, the winner is the player that remains alive, or that which owns
more ships if more than one survives.  

There are two strong constraints that determine the possible methods
to apply in order to design a bot that can play and win this game: the time for making a decision is \textit{just one second},  
and the bot is \textit{not allowed to store any kind of information}
about its former actions, about the opponent's actions or about the
state of the game (i.e., the game's map).  
% Is this used any way in this context? 
% Antonio - claro, son las restricciones que tiene el problema. 
% Una cosa, ¿no podemos hablar en español entre autores españoles? Vamos a ser pragmáticos.

Therefore, the objective of this paper is to generate a reliable DT
% DT? - JJ
% Antonio - Decision Tree, lee la intro
for a bot, which, according to the state of the map in every simulated
turn (input), returns a set of actions to perform in order to fight
the enemy, conquer its resources, and finally, win the game.  


%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% STATE OF THE ART %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------
\section{State of the art}
\label{sec:soa}

RTS games have been used extensively in the Computational Intelligence (CI) area (see \cite{Lara2013review} for a survey). 
Among other techniques, EAs have been widely used as a CI method in RTS games, for example, for parameter optimisation \cite{Genebot_CEC11,ExpGenebot_CIG2012}, building order decision \cite{Kostler2013:MO_StarCraftII,Barriga2014:BuildingOrder_GA}, learning \cite{learning_StarCraft_AAIDE11,Wender_RL_CIG12}, or content generation \cite{Mahlmann2012MapGeneration,Lara_EntComp_PCG_RTS14}. 

GP has also proved to be a very good tool for developing strategies in games, achieving results comparable to human, or human-based competitors \cite{Sipper2007gameplaying}. They also have obtained higher ranking than solvers produced by other techniques or even beating expert human players \cite{Elyasaf2012FreeCell}. GP has also been used in different kind of games, such as board-games \cite{Benbassat2012Reversi}, (apparently) `simple' games like Ms. Pac-Man \cite{Brandstetter2012PacMan} or Spoof \cite{Wittkamp2007spoof}, and even in modern videogames such as First Person Shooters (FPS) as Unreal\texttrademark~ \cite{Mora_UnrealBots10,Esparcia2013GPunreal}. 

With respect to RTS games, it has not been extremely exploited, so there are just a few applications, such as pathfinding \cite{pathfinding_GP_RTS}, definition of tactics in an abstract tactical game \cite{KeaveneyO09_GP_RTS},and recently to the automatic generation of strategies for a bot \cite{Garcia15Starcraft}.
% Antonio - Fergu, mira a ver si quieres añadir algo a esto. ;)
In this paper, the aim is to apply GP inside a `simple' RTS, Planet Wars, in order to define the whole behavioural engine for bot.
 
Planet Wars, the game used in this work, has also been used in other
researches as an experimental framework for agent testing \cite{ZiolkoK12_PW_Reasoning}.  
% references! - JJ
% Antonio - vienen ahora
% Antonio - TODO: Anonimizar las referencias. Lo demás se puede dejar así, si se quiere
We have worked on this game in some previous studies
\cite{Genebot_CEC11,genebot-evo12,Co-Genebot_EVO2014}, mainly applying
Genetic Algorithms for evolving (the parameters of) a behavioural
engine previously defined by a human expert from scratch.   
% Nobody else has ever used Plante Wars? You have to prove it is popular - JJ
% Antonio - He encontrado sólo a ese hombre, pero es una IA hecha a manubrio. Que yo sepa, sólo los de Málaga y nosotros trabajamos actívamente y ellos en PCG, no en agentes.
% Por otra parte, ¿por qué hay que demostrar que es popular? Es un RTS simple y nos sirve de banco de pruebas, con eso creo que basta...

% Antonio - TODO: anonimizar referencias
We used GP in a previous approach for Planet Wars bots
\cite{Garcia14Treedepth}, as a following step to those works, to avoid
the strict limitations that the initial bots had, such as the fixed
behavioural structure (a Finite State Machine), which just offers a
few degrees of improvement, namely the evolved set of eight
parameters.
Thus, GP was applied to create the Decision Tree (DT) that the bots used to make decisions during the game, and the resulting bots were able to beat ANONIMOUSBOT, %GeneBot \cite{Genebot_CEC11}, 
our initial bot improved by means of GAs.
This paper goes a bit further and analyses the evolution of the DTs themselves along generations, in order to show how do they change and how the conditions and actions are distributed on the trees. Moreover, this study lets us identify the type of strategies that the best bots follow, i.e. we can extract some {\em automatically generated knowledge} (or tips) that could help other players to improve their skills in the game.

However, as it is usual when EAs are applied \cite{Jin2005303,QianYZ13}, and also due to the pseudostochastic nature of games, the evolution of bots is sensitive to {\em uncertainty} or {\em noise} \cite{merelo14:noisy}, even in a `simple' game as Planet Wars \cite{Mora_noisy_jcst}.
Basic strategies to handle noisy fitness functions include using a larger population size, averaging to filter out the noise (re-sampling) \cite{Branke98_robust}, thresholding (employing a threshold value to be used in a selection operator for noisy fitness functions) \cite{Markon2001_thresholding}, or changing the selection criteria.

In this paper we also propose testing three different fitness functions,
with the aim of checking, in addition to just their raw performance,
which one if less sensitive to this kind of noise. This study will be useful to other researchers in order to select the best type of evaluation function, when defining other evolutionary methods working on RTSs.

%This is not a state of the art. A state of the art says where the
%current best result in this area is in order to prove that this one
%is better. What is the current best result in this area? - JJ

% Antonio - no estoy de acuerdo, no se trata siempre de mejorar unos resultados, se puede tratar de ofrecer información adicional, complementar lo que hay con otro tipo de análisis. Esto va en esa línea, se prueban varias funciones de distinto tipo, intentando decidir cuál es más adecuada, por si se puede generalizar a otros RTSs. Además, el estudio de los árboles generados, es algo complementario, que no he visto antes (al menos de esta forma) y menos en este entorno. Eso ya de por sí, tiene interés por el conocimiento que se puede extraer de ahí y no mejora resultados... 
% Todo esto de cómo debe ser un artículo y un SotA, no es "sota, caballo, rey", he leído trabajos de todo tipo y publicados en sitios relevantes. Simplemente son puntos de vista distintos de lo que es avanzar en esta ciencia para unos y para otros.
% Antonio - Conclusión, yo lo veo bien. Lo he aclarado un poco más en el texto para dejar patente esto que digo...
% Antonio - Si los demás no lo veis así y queréis cambiarlo, adelante. ;)

%-------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% GP BOT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------
\section{GPBot}
\label{sec:agent}


The Genetic Programming-based bot or {\em GPBot} \cite{Garcia14Treedepth} evolves a set of rules which, in turn, models a Decision Tree (DT).

During the evolution, every individual in the population (a tree) must be evaluated. To do so, the tree is set as the behavioural engine of an agent, which is then placed in a map against a rival in a Planet Wars match. Depending on the obtained results, the agent (i.e. the individual) gets a fitness value, that will be considered in the evolutionary process as a measure of its validity. 
 
Thus, during the match the tree will be used (by the bot) in order to select the best strategy at every moment, i.e. for every planet a target will be selected along with the number of ships to send from one the other.

\noindent The used DTs are binary trees of expressions composed by two different \textit{types of nodes}:

\begin{itemize}
\item {\em Decision}: a logical expression formed by a variable, a less than operator ($<$), and a number between 0 and 1. It is the equivalent to a ``primitive'' in the field of GP.
\item {\em Action}: a leave of the tree (therefore, a ``terminal''). Each decision is the name of the method to call from the planet that executes the tree. This method indicates to which planet send a percentage of available ships (from 0 to 1). 
\end{itemize}

\noindent The decisions are based on the values of different \textit{variables} which are computed considering some other variables in the game. They are defined by a human expert, and are:

\begin{itemize}
\item {\em myShipsEnemyRatio}: Ratio between the player's ships and enemy's ships.
\item {\em myShipsLandedFlyingRatio}: Ratio between the player's landed and flying ships.
\item {\em myPlanetsEnemyRatio}: Ratio between the number of player's planets and the enemy's ones.
\item {\em myPlanetsTotalRatio}: Ratio between the number of player's planet and total planets (neutrals and enemy included).
\item {\em actualMyShipsRatio}: Ratio between the number of ships in the specific planet that evaluates the tree and player's total ships.
\item {\em actualLandedFlyingRatio}: Ratio between the number of ships landed and flying from the specific planet that evaluates the tree and player's total ships.
\end{itemize}

\noindent Finally, the possible \textit{decisions} are:

\begin{itemize}
\item {\em Attack Nearest (Neutral|Enemy|NotMy) Planet}: The objective is the nearest planet.
\item {\em Attack Weakest (Neutral|Enemy|NotMy) Planet}: The objective is the planet with less ships.
\item {\em Attack Wealthiest (Neutral|Enemy|NotMy) Planet}: The objective is the planet with higher growth rate.
\item {\em Attack Beneficial (Neutral|Enemy|NotMy) Planet}: The objective is the  more beneficial planet, that is, the one with highest growth rate divided by the number of ships.
\item {\em Attack Quickest (Neutral|Enemy|NotMy) Planet}: The objective is the planet easier to be conquered: the lowest product between the distance from the planet that executes the tree and the number of  ships in the objective planet.
\item {\em Attack (Neutral|Enemy|NotMy) Base}: The objective is the planet with more ships (that is, the base).
\item {\em  Attack Random Planet}.
\item {\em Reinforce Nearest Planet}: Reinforce the nearest player's planet to the planet that executes the tree.
\item {\em Reinforce Base}: Reinforce the player's planet with higher number of ships.
\item {\em Reinforce Wealthiest Planet}: Reinforce the player's planet with higher grown rate.
\item {\em Do nothing}.

\end{itemize}

\noindent An example of a possible DT is shown below. This example tree has a total of 5 nodes, with 2 decisions and 3 actions, and a depth of 3 levels.

\begin{verbatim}

if(myShipsLandedFlyingRatio < 0.796)
   if(actualMyShipsRatio < 0.201)
      attackWeakestNeutralPlanet(0.481);
   else
      attackNearestEnemyPlanet(0.913);
else
   attackNearestEnemyPlanet(0.819);

\end{verbatim}\\\\

\noindent The bot's behaviour is explained in Algorithm \ref{alg:turn}.

\begin{algorithm}[ht]
\begin{algorithmic}
%\SetAlgoLined
%\KwData{this text}
%\KwResult{how to write algorithm with \LaTeX2e }

\STATE // At the beginning of the execution the agent receives the tree
\STATE tree $\leftarrow$ readTree()
\WHILE{game not finished}
	\STATE // starts the turn
	\STATE calculateGlobalPlanets() // e.g. Base or Enemy Base
	\STATE calculateGlobalRatios() // e.g. myPlanetsEnemyRatio
	\FOR{Each p in PlayerPlanets}
		\STATE calculateLocalPlanets(p) // e.g. NearestNeutralPlanet to p
		\STATE calculateLocalRatios(p) //e.g actualMyShipsRatio
		\STATE executeTree(p,tree)  // Send a percentage of ships to destination
   \ENDFOR
\ENDWHILE

\end{algorithmic}
\caption{Pseudocode of the proposed agent. The same tree is used during all the agent's execution}
\label{alg:turn}
\end{algorithm}


Next section explains the three different fitness functions that have been implemented to evaluate the agents' performance during the battles. 


%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% FITNESS FUNCTIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------

\section{Fitness Functions}
\label{sec:fitness_functions}

% ---------------------------------------------------------------------

Fitness function should have some characteristic in order to ensure a good performance of the algorithm where it is used: 
\begin{itemize}
\item create a smooth landscape: yield close values in the evaluation of close/similar individuals.
\item being readily available: every correct individual can always be evaluated.
\item being robust: the evaluation of a specific individual should be always the same (or similar, at least).
\item being reliable: a good individual (according to the problem objective) must get a higher fitness value than a bad one.
\end{itemize}
% Antonio - estas son las características que hemos pensado, si hay alguna más, por favor, metedla. ;)

This section describes three different fitness functions to be applied in the GP algorithm described in Section \ref{sec:agent}, in order to generate competitive bots for Planet Wars.

Thus, subsection \ref{subsec:fitness_turns} presents a hierarchical fitness based on the number of victories, and also the number of turns of the battles for tiebreaks. 
Subsections \ref{subsec:fitness_slope} and \ref{subsec:fitness_area} show two fitness functions which take into account the behaviour of the agents during the battles, i.e. they consider the percentage of resources (ships) gathered by the bot, and compute, respectively, a slope or an area as its evaluation value.
% Antonio - He reescrito esta intro, creo que ahora está mejor.

% ---------------------------------------------

\subsection{Fitness based on Victories}
\label{subsec:fitness_turns}

The justification of this fitness is to reach the main objective we pursuit with this method: generate bots able to win as many times as possible.
% We are mainly interested in a bot that is able to win as many times
% as possible, that is why... - JJ
% Antonio - dicho
This is an improvement of the hierarchical fitness considered in \cite{Genebot_CEC11}.
In this approach, an individual is better than another if it wins in a larger number of maps. In case of equality of victories, then the individual with more turns to be defeated (i.e. the stronger one) is considered as better. 
Every evaluation consists in five battles against the same enemy, ANONIMOUSBOT
% Antonio - TODO: anonimizar referencia
 \cite{Genebot_CEC11}, so the maximum fitness value with this approach is 5 victories and 0 turns. 
For two bots, A and B, the fitness comparison (and therefore, their order inside the population) is defined as Algorithm \ref{alg:fitness_turns_positions} shows.

\begin{algorithm}[ht]
\begin{algorithmic}
        
\STATE $A,B \in Population$
\IF{A.victories $=$ B.victories}
	\IF{A.turns $>=$ B.turns}
		\STATE A is better than B
	\ELSE
		\STATE B is better than A
	\ENDIF
\ELSE
	\IF{A.victories $>$ B.victories}
		\STATE A is better than B
	\ELSE
		\STATE B is better than A
	\ENDIF
\ENDIF

\end{algorithmic}
\caption{Comparison between two individuals using fitness based on victories}
\label{alg:fitness_turns_positions}
\end{algorithm}

In this fitness, we are only interested in the final result of the battles (position and number of turns). We do not include in the computation how the bot has reached them. The problem of this function is that the consideration of two different terms (victories and turns) makes it difficult the comparison between different evaluations, and it is hard to decide which individual is better if both have won just some of the battles. 

Thus, in this work, two additional evaluation functions have been proposed in order to make a, in principle, fairer comparison between bots, trying to add another factor in order to be less sensitive to noise \cite{Mora_noisy_jcst}. 
Both of them are based on the percentage of ships belonging to each player in every turn. They are normalised considering the total amount of ships in the game for that turn (including neutrals ships in neutral planets), so for each player, there is a different {\em cloud of ships}.
In the next subsections there are described the two alternatives to transform this cloud of points into a singular value for the fitness function: the use of slopes (statistical method) and areas (numerical method).

% ---------------------------------------------------------------------

\subsection{Fitness based on Slope}
%  "slope based fitness"   [pedro]
\label{subsec:fitness_slope}

In this case, a square regression analysis is computed in order to transform the cloud of points into a simple line. The line is represented as {$y = \alpha \times x + \beta $}, where {$\alpha$} and {$\beta$} are calculated as shown in Equations \ref{eq:alpha} and \ref{eq:beta}, computing a least squares regression. For every bot in the simulation we calculate $\alpha$ and ($slope$). This $slope$ is the fitness of every bot for that simulation. 

%A graphical example can be seen in Figure \ref{figura:nubecita:pendiente}.

\begin{equation}
\label{eq:alpha}
        \alpha = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})(Y_{i} - \bar{Y_{i}})}{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})^{2}}
\end{equation}

\begin{equation}
\label{eq:beta}
        \beta = \bar{Y}-\alpha\bar{X}
\end{equation}


%\begin{figure}[ht]
%\centering
%\hspace*{-1in}
%  \epsfig{file=imags/nubecita_pendiente.pdf,width=0.6
%  \textwidth}
%\caption{Fitness based on Slope: number of ships of every bot in each turn}
%\label{figura:nubecita:pendiente}
%\end{figure}


% \begin{figure}[ht]
% \centering
% \hspace*{-1in}
% \begin{subfigure}[H]{0.4\textwidth}
% 	\large
%    \begin{equation}
%        \alpha = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})(Y_{i} - \bar{Y_{i}})}{\sum_{i=1}^{n}(X_{i} - \bar{X_{i}})^{2}}
%    \end{equation}
%    \begin{equation}
%        \beta = \bar{Y}-\alpha\bar{X}
%    \end{equation}
%    \caption{Least Squares Regression}
%    \label{equation:LeastSquares}
% \end{subfigure}
% \hfill
% \hspace*{0.2in}
% \begin{subfigure}[H]{0.7\textwidth}
% \begin{center}
%  \epsfig{file=imagenes/nubecita_pendiente.eps,width=1.1\textwidth}
% \end{center}
% \caption{Number of ships of every bot in each turn} %Maribel, cambio if por of
% \label{figura:nubecita:pendiente}
% \end{subfigure}
% \caption{Fitness based on Slope}
% \end{figure}

Theoretical maximum and minimum values are set for this fitness. An optimum bot that wins in the first turn, has an ideal slope of {$1$}, so this is the maximum value of our fitness. On the other hand, a bot that loses in the first turn,  has a slope of {$-1$}. Thus, if we calculate the $slope$, we know if the bot {$WINs$} ({$slope>0$}) or {$LOSEs$} {$slope<0$}. 
The values of the five different battles are summed to compute the global $slope$. Then, the bot with the highest value will be the best is each simulation or battle. 

% ANTARES: Este método es el que mejor permite puntuar un invidiuo al menos teóricamente. Un invidiuo que consiga la vitoria de la contienda conseguirá una puntuación positiva (pendiente mayor que cero), y será más alta cuanto mejor haya sido. Si pierde obtendrá una puntuación negativa (pendiente menor que cero) y se perderá más "puntos" (en el fitness) cuanto peor haya sido el resultado de la contienda.

% Antonio - Traducido lo que dice Antares
This method, theoretically, assigns the fairest value to an individual, since the obtained result is highly correlated with the victory or lose in the battle, being it positive or negative, respectively. Moreover, the value will be higher if the number of resources/ships gathered has been higher, and the other way round. 


% ---------------------------------------------------------------------

\subsection{Fitness based on Area}
% "area based fitness"    [pedro]
\label{subsec:fitness_area}

In this function, the integral of the curve of the bot's live-line is used for calculating the area that is `covered' by the fitness cloud of points (see Equation \ref{eq:area}). This {$area$} is normalised considering the number of turns, and thus it represents the average percentage of owned ships during the battle for each player. 
%An example is shown in Fig. \ref{figura:nubecita:area}.

\begin{equation}
        area=\frac{\int_{0}^{t}\%ships(x)dx}{t}
    \label{eq:area}
\end{equation}

% \begin{figure}[h]
% \begin{center}
%   \epsfig{file=imagenes/nubecita_integral.eps,width=0.7\textwidth}
% \end{center}
% \caption{Fitness based on Area. Example of area under the live-line curve.}
% \label{figura:nubecita:area}
% \end{figure}

%\begin{figure}[h]
%\centering
%\hspace*{-1in}
%\begin{subfigure}[H]{0.4\textwidth}
%	\large
%    \begin{equation}
%        area=\frac{\int_{0}^{t}\%ships(x)dx}{t}
%    \end{equation}
%    \caption{Calculus of the area}
%    \label{equation:area}
%\end{subfigure}
%\begin{subfigure}[H]{0.6\textwidth}
%\begin{center}
%  \epsfig{file=imagenes/nubecita_integral.eps,width=0.6\textwidth}
%\end{center}
%\caption{Example of area under the live-line curve.} 
%\label{figura:nubecita:area}
%\end{subfigure}
%\caption{Fitness based on Area}
%\end{figure}

As in previous case, maximum and minimum values have been set for this fitness. If an optimal bot wins in the first turn, the area of each live-line is close to {$1$}, so this is the maximum value of the fitness. Otherwise, if a bot loses in the first turn, its live-line area is close to {$0$}. 

In this case, we do not extract additional information about which bot wins the battle, because the area of the live-line is not related with the winner of the battle. Thus, some information is lost. 
% ANTARES: Este método no permite conocer si un individuo ha ganado o perdido la contienda, ya que el valor númerico del mismo no lo refleja de ninguna manera. A pesar de esta pérdida de información, es el método que mejor puede romper casos donde un fitness jerárquico pueda parecer injusto. ¿Es mejor un individuo que solo gana una vez y el resto de veces pierde muy rápidamente? ¿O uno que pierde pero tarda mucho tiempo, pues no le resulta sencillo al rival vencerle?
% Antonio - Traduzco
However, in spite of the missed information, this method is better than slope in order to differentiate between bots which have an intermediate value for the hierarchical fitness. Thus, the assigned value with this function is useful to decide if it is better a bot which wins sometimes but when it loses, the match last just a few turns; or a bot which loses many times, but the matches takes a large number of turns.


%-----------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% EXPERIMENTAL SETUP %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-----------------------------------------------------------------------

\section{Experimental Setup}
\label{sec:experiments}

Sub-tree crossover and 1-node mutation evolutionary operators have
been used, following other researchers' proposals that have used these
operators obtaining good results \cite{Esparcia2013GPunreal}. In this
case, the mutation randomly changes the decision of a node or mutate
the value with a step-size of 0.25 (an adequate value empirically
tested). Each configuration is executed 21 times, with a population of
32 individuals and a 2-tournament selector for a pool of 16
parents.

%This is not experimental setup, it is description of
%parameters used in the algorithm. Experimental setup is
%number of runs, which conditions, so on - JJ
% Antonio - se describe también eso

To evaluate every individual during the evolution, a battle with a previously created bot is performed in 5 different (but representative) maps provided by Google. 
%Hierarchical fitness is used, as proposed in \cite{Genebot_CEC11}. Thus, an individual is better than another if it wins in a higher number of maps. In case of equality of victories, then the individual with more turns to be defeated (i.e. the stronger one) is considered better. The maximum fitness is, therefore 5 victories and 0 turns. 
Also, and due to the presence of noise (different evaluations will yield different values), all individuals are re-evaluated in every generation, constituting an implicit averaging as it is suggested by the authors in \cite{Jin2005303,DBLP:conf/ijcci/MereloLFGCCRMG15}.
% 1. You could cite our papers on noise
% 2. You could USE the conclusions of those papers. In this case: ?use
% all evaluations in all generations! - JJ
% Antonio - ya está citado. ;)

A publicly available bot has been chosen for the
% Antonio - TODO: anonimizar
experiments\footnote{It can be downloaded from \url{https://github.com/deantares/genebot}}. The bot to confront is {\em ANONIMOUSBOT}, proposed in \cite{Genebot_CEC11}. As stated, this bot was trained using a GA to optimise the eight parameters that conforms a set of hand-made rules, obtained from the experience of an expert human player.  Table \ref{tab:parameters} shows the values of the parameters considered.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
\hline
{\em Parameter Name} & {\em Value} \\\hline \hline
Population size & 32 \\\hline
Crossover type & Sub-tree crossover \\ \hline
Crossover rate & 0.5\\ \hline
Mutation  & 1-node mutation\\ \hline
Mutation step-size & 0.25 \\ \hline
Selection & 2-tournament \\ \hline
Replacement & Steady-state\\ \hline
Stop criterion & 50 generations \\ \hline
Maximum Tree Depth & 7  \\ \hline %FERGU: quitados distintos tamaÃ±os
Runs per configuration & 30 \\ \hline
Evaluation & Playing versus ANONIMOUSBOT \cite{Genebot_CEC11}  \\ \hline 
Maps used for evaluation & map76, map69, map7, map11, map26 
\\ \hline
\end{tabular}
\caption{Parameters used in the experiments}
\label{tab:parameters}
\end{center}
\end{table}


After all the executions we have evaluated the obtained best individuals in all runs confronting to the other bots in a larger set of maps, 
% Antonio - ¿en cuántos mapas? Poner el número concreto
in order to study the behaviour of the algorithm and how good are the obtained bots versus enemies and maps that have not been used for training.

The algorithm has been implemented using the OSGiLiath, a service-oriented evolutionary framework \cite{Garcia13Service}. 
The generated tree is compiled in real-time and injected in the agent's code using Javassist \footnote{\url{www.javassist.org}} library. All the source code used in this work is available under a LGPL V3 License in 
% Antonio - TODO: anonimizar
\url{http://www.osgiliath.org}.


%-------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% RESULTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%-------------------------------------------------------------
\section{Analysis of Results}
\label{sec:results}

\subsection{Study of the GP algorithm and Fitness Functions}
\label{subsec:evolutionary_algorithm}

Table \ref{tab:results3config} shows the obtained results after
executing each approach (GP + fitness function) 21 times. 
Although the fitness are not comparable, as they obviously apply different metrics, the \textit{Victory}-based fitness achieves values near to the optimum (5) at the end of the run (look at the best individual and average population values). The \textit{Area}-based and \textit{Slope}-based fitness yield results under their theoretical optimum, as they depend on more information ranges (variation in the number of ships). 

\begin{table*}
\centering{
\begin{tabular}{|c|c|c|} \hline            
		& Average best fitness	&	Average population fitness	\\ \hline \hline
Victory	& 4.761 $\pm$	0.624	&	4.345	$\pm$	0.78 \\ \hline
Area	& 2.838	$\pm$	1.198	&	2.347	$\pm$	0.949 \\ \hline
Slope	& 2.296	$\pm$	0.486	&	2.103	$\pm$	0.434 \\ \hline

\end{tabular}
\caption{Average results obtained for each approach at the end of the 21
  runs}
%Don't understand this. How do you compare fitnesses if they are
%computed in a different way? Shouldn't you call it score? Or
%victories? Or something else? - JJ
% ANTARES: See next section ;)
\label{tab:results3config}
}
\end{table*}

However, as these values are not comparable, we have conducted a graphical study of the evolution of fitness on the runs.
Figure \ref{figura:fitness_turns} shows how the fitness of the best individuals and also the average fitness of the population are improved along generations, as it is desired in an EA. Figures \ref{figura:fitness_area} and \ref{figura:fitness_slope} show the same tendencies. Moreover, as expected, the fitness of the best individuals have a higher progression than averages.
These facts mean that the fitness function are correctly defined and properly working.

Comparing the results for every fitness with the rest, it can be seen that Victory fitness shows a higher progression than the others. This is due to the range of the dominions of values in these functions. However, it can be noted that the variability of values is higher in area than in Slope, because of the negative values that can be yield in the latter.
% Antonio - es por esto? Que estoy medio dormido. :(

As it can be seen in the dispersion of the cloud of points, the noise if present in all the functions, but Area shows a better tolerance to it, since the cloud is a bit narrower than the others.
% Antonio - Y esto? :D

% Antonio - TODO: comentar más cosas si se os ocurren

\begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_turns.pdf}
 \end{center} % no podÃ©is empezar una frase con "for
              % individuals". Empezad con Boxplot - JJ TambiÃ©n es
              % inÃºtil un boxplot sÃ³lo, sÃ³lo sirve para comparar. - JJ
 \caption{\textbf{Victory-based Fitness} individuals: Boxplot of the fitness of the \emph{Best individuals}, one per run, and evolution of the \emph{Best fitness} and \emph{Average population fitness} of every execution. In the evolution plots (centre and right), single points and locally weighted polynomial curve (loess) are shown.}
 \label{figura:fitness_turns}
 \end{figure}
% Antonio - 'Loess' es correcto?

   \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_area.pdf}
 \end{center}
 \caption{\textbf{Area-based Fitness} individuals: Boxplot of the fitness of the \emph{Best individuals} of every execution and evolution of the \emph{Best fitness} and \emph{Average population fitness} of every execution. In evolution plots (centre and right), single points and locally weighted polynomial curve (loess) are shown.}
 \label{figura:fitness_area}
 \end{figure}

    \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_slope.pdf}
 \end{center}
 \caption{\textbf{Slope-based Fitness} individuals: Boxplot of the fitness of the \emph{Best individuals} of every execution and evolution of the \emph{Best fitness} and \emph{Average population fitness} of every execution. In evolution plots (centre and right), single points and locally weighted polynomial curve (loess) are shown.}
 \label{figura:fitness_slope}
 \end{figure}



Looking at the evolution of individuals (trees), Figure \ref{figura:evolutionDEPTH} shows the average depth of the trees along the generations. % Antonio - muestra la profundidad media o la de los mejores?
The three approaches show a similar behaviour in this respect, i.e. the average depth grows in every generation. Victory fitness approach is the one with a smaller growing. Thus, it obtains smaller trees that are very good individuals anyway.

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/evolution_AVERAGE_DEPTH.pdf}
 \end{center}
 \caption{Evolution of the average depth of every run by fitness
   method}
 \label{figura:evolutionDEPTH}
 \end{figure}

Figure \ref{figura:evolutionAGE} plots the average age of the individuals in the population, i.e. the number of generations they survive. A constant growth in this graph will mean that the individuals are growing in age, so they are not being replaced, and the population is not being improved properly.
In the graphs we cannot appreciate a growing tendency after generation 10 for Victory and Slope, which means these functions a working well. However, Area behaves a bit different and there is present a slightly progression. This is another evidence that this method is less sensitive to noise, and good individuals continue being good along generations. 
% Antonio - Yo creo que esto es así, ¿qué opináis? :P

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/evolution_AVERAGE_AGE.pdf}
 \end{center}
 \caption{Evolution of the average age of the population in every run by fitness
   method} 
 \label{figura:evolutionAGE}
 \end{figure}


% -----------------------------------------------------------

\subsection{Fitness Benchmark}
\label{subsec:fitness_benchmark}

Even though the Victory-based fitness yields better results
(near optimal), to do a fair comparison, we have confronted the 21
best bots obtained with each configuration (one per run) against
ANONIMOUSBOT. However these battles have been performed in 9 maps that were
different to the ones used to evolve the bots. These maps, provided by Google, are considered as representative \cite{ExpGenebot_CIG2012}, because they have different features to promote a wide set of strategies, i.e. different distributions of planets, sizes and number of initial ships. 

This experiment has been conducted in order to validate if the bots obtained by the proposed approaches can be competitive in terms of quality in maps not used for evolution/evaluation. Results are shown in Figure \ref{figura:boxplotvictoriesgenebot}. As it can be seen, the Victory-based fitness achieves significantly better results than the other methods.
This could be explained considering the way the fitness is computed, because Victory fitness tends to promote to its generated bots, obviously, just obtaining the victory. Area and Slope also take care of the number of ships generated during the battles, which may lead to a more `conservative' (less aggressive) behaviour. This would be tested in section \ref{subsec:conditions_actions}.

\begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/vsGENEBOT_Boxplot.pdf}
 \end{center}
 \caption{Boxplot of percentage of victories of the bots obtained by each approach vs ANONIMOUSBOT. 9 different battles have been performed per bot in different maps.} 
% Antonio - la leyenda de la figura no debería mencionar a GeneBot por lo de la anonimización, creo yo. O debemos quitar ANONIMOUSBOT y poner GeneBot en todos los sitios... :_(
 \label{figura:boxplotvictoriesgenebot}
 \end{figure}

In order to complement these results an additional experiment has been conducted, making a direct comparison between the three methods. To this end, each one of the 21 best individuals obtained per approach has been tested competing against all the rest (in a 1 vs 1 battle) in 9 matches per pair of bots, one per representative map.
%
%Los mapas son & 7 & 11 & 13 & 26 & 32 & 64 & 69 & 76 & 87 \ y os explico un poco las cuentas.
%Se han hecho 35721 batallas en 9 mapas con 63 bots (21 por mÃ©todo).
%Cada bot se ha enfrentado con los otros bots en 9 mapas (63 enfrentamientos) mÃ¡s luego Ã©l mismo ha servido como rival para los otros bots (otros 63 combates)
%
This allows a comparison with a bigger number of bots, and also,
allows the analysis of their behaviour against rivals not previously
used during training (as in the experiment above). 

The boxplots of the percentage of victories from the best bots obtained by each method are shown in Figure \ref{figura:boxplotvictories}. It is clear from the image that the Slope fitness does not get good results with respect to the other methods. This could happen due to this approach is the most sensitive to noise, so, maybe the best individuals generated are not actually so good.
% Antonio - os parece correcta esta justificación?

% Antares: Supongo que lo querrÃ¡ decir, es que el fitness basado en pendiente suma puntos si gana (la pendiente es positiva) o resta si pierde (la pendiente es negativa). AsÃ­ que supongo que lo que se quiere decir es que la puntuaciÃ³n no se sabe cuanto ha sumado y cuando ha restado. Igual a sumado mucho pero ha restado algo. O igual ha sumado muy poco. No sÃ© a que otra cosa se puede referir.

The Victory-based fitness achieves better results in average, being also more robust (small standard deviation). However, looking at the Area fitness results, they outperform several times the Victory results, obtaining more victories.
% Antonio - ¿Esto cómo se justifica que no se me ocurre?

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/batallas_Boxplot.pdf}
 \end{center}
 \caption{Boxplot of average percentage of victories of the best bots obtained by each method vs. the rest.}
% Antonio - hay que corregir el título, debería poner algo como "Victories of everyone of the best individuals per execution versus the rest of best", pero no "each best individuals"
 \label{figura:boxplotvictories}
 \end{figure}


% There is still a final remark, which concerns to the percentage of draw matches.
% Antonio - hay que decir algo de los empates? La tabla se había quitado...

%--------------------------------------------------------

\subsection{Conditions and Actions analysis}
\label{subsec:conditions_actions}

% Please don't write anything in Spanish in the final version - JJ
% This title does not say anything and is not related to the title of
% the paper. It should be a different paper. - JJ
%Pablo: changed title. We cannot remove this section because is the "extension" required to be published in the Special Issue :)

%Hay que comentar que se ha realizado un estudio de que acciones y condiciones de las descritas en la sección 4 del artículo, que han sido definidas por un experto. Esto permite discernir que elementos del juego que ha definido el experto son más "útiles" dentro del desarrollo del juego, estudiando que aciones y condiciones son más empleadas por los bots que han sido generados y optimizados mediante programación genética. Esta información puede ser útil para redefinir las información proporcionada por el experto. 

In this section, we present a study on the distribution and appearance of the decisions and actions described in Section \ref{sec:agent}. This could allow to detect which elements that model the behaviour of the bots are more important during the game process, analysing the number of actions and decisions, and comparing their rate using the different compared fitness functions. This information can be useful to redefine the information used during the evolutionary process, such as the conditions and actions defined by the expert themselves, if they are not useful, according to this study. 

%Para ello se han estudiado los árboles que se han generado como mejor individuo de cada generación, de cada ejecución, de cada uno de los tres métodos de fitness propuestos anteriormente. Se estudia, en concreto, el porcentaje utilización de un "nodo del árbol" concreto en el total de nodos del mismo tipo empleados por los inviduos que comparten esa misma generación.
%Un nodo más "útil" para conseguir la victoria tendrá una mayor presencia a medida que el algoritmo evolutivo avanza generaciones. De igual manera, un nodo "poco útil" o "menos útil" para conseguir la victoria tendrá menos presencia en los mejores individuos a media que converge el algoritmo evolutivo a mejores soluciones.

The analysis has been conducted on the best individuals obtained in each generation of the three fitness methods, counting the percentage of use of a decision/action with respect to the total of the same type, taking into account all the best individuals of that generation. Therefore, a more useful node to win the match will have more presence in the best individuals during the algorithm run, and vice versa.

%En primer lugar en la figura \ref{figura:e_number_nodes} se muestra el crecimiento del número promedio de condiciones y acciones a medida que evolucionan los individuos. Este hecho ya se había mostrado en la figura \ref{figura:evolutionDEPTH} en el que se presenta la profundidad el árbol generado. Dado que los árboles de decisiones son binarios perfectos, un crecimiento de la profundidad del árbol implica un aumento de las condiciones (nodos internos del árbol) y las acciones (nodos externos del árbol)

Figure \ref{figura:e_number_nodes} shows the average growing of conditions and actions during the evolution. As described before in Figure \ref{figura:evolutionDEPTH}, when the number of generations increases, the depth also does, as well as, obviously, the number of nodes. But in this case, in the latter generations of the Victory fitness the number of nodes increases more than in the other two fitness methods, being the depth of this method also lower than the Slope and Area. This indicates that the Victory Fitness is generating more balanced trees than the other methods.

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_number_nodes_s.pdf}
 \end{center}
 \caption{Evolution of the number of nodes of the best individual by generation of every execution. Nodes can be conditions (internal nodes) or actions (leaf nodes). The tree is always a binary tree, so both (conditions and actions) are related.}
 \label{figura:e_number_nodes}
 \end{figure}

%Existen seis tipos de condiciones que han sido descritas en la sección 4. La figura \ref{figura:e_conditions} muestra la evolución del porcentaje de condiciones empleadas en cada generación. Se muestra como la regla \emph{actualMyShipsRatio} tiene un aumento de presencia en los mejores individuos frente a las otras condiciones, que se mantienen constantes en su uso o decrecen.
%Esta información puede servir para un diseñador experto para discernir que reglas son más interesante explotar.

Regarding the {\em condition or decision type}, Section \ref{sec:agent} described six types of ratios evaluated in every planet to decide which next branch of the tree should be selected. Figure \ref{figura:e_conditions} shows the evolution of these types of ratios. Results show that there is an increase in the Victory fitness in the rule \emph{actualMyShipsRatio}, the one that compares the amount of ships in the planet executing the tree with the player's total amount of ships. This makes sense, as the tree is executed for each player's planet and the percentage of ships used at the end of the tree also depends of that current planet. However, with the other two methods, the most used decision compares the number player's ships with the enemy's. 
% Antonio - esto que he puesto es correcto, verdad? Se comparan las naves del jugador con las del enemigo, no?
As those fitness try to use the number of ships as a quality measure, they are generating trees to maintain a higher number of ships than the rival during the whole match, instead focusing in a quick victory.

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_CONDITIONS_s.pdf}
 \end{center}
 \caption{Evolution of the percentage of every condition over the total of conditions used by the best individual by generation of every execution.}
 \label{figura:e_conditions}
 \end{figure}

%La figura \ref{figura:e_actions} muestra entre los tres tipos de acciones posibles (attack, reinforce or doNothing) que puede elegir realizar el agente, la que tiene una mayor presencia es la de realizar ataque, que tiene un crecimiento constante en decrimento de doNothing. La acción de refuerzo sufre un decrecimiento a medida que mejoran los nodos aunque permaneciendo constante al final, por lo que sigue siendo empleada. La acción de doNothing, que era experable que terminase convergiendo a cero, sigue teniendo presencia incluso en las últimas generaciones del algoritmo. Esto es debido, a que en ocasiones, no hacer nada y experar (por ejemplo, cuando no se puede conquistar ningún planeta) puede ser una estrategia útil.

With respect to the {\em action type} that an agent can choose, Figure \ref{figura:e_actions} shows the percentage of the three types of actions that can be performed by the agent: {\em attack}, {\em reinforce} and {\em doNothing}. Results clearly show that the {\em attack} action is the most used in all fitness methods, but it is interesting to mention that the Victory fitness uses less times than the others the {\em reinforcement} one with respect to attacking. This also can be explained because the number of ships is not measured during the evaluation, unlike the other two methods, where a high amount of ships during all the run yields a higher fitness value.

%This should go to a different paper. This paper is about studying
%fitness. If you insert this, you should do it for _every_ fitness - JJ
% Antonio - se ha hecho por fitness. ;)

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_ACTIONS_s.pdf}
 \end{center}
 \caption{Evolution of the percentage of every type of actions over the total of actions used by the best individual by generation of every execution.
 }
 \label{figura:e_actions}
 \end{figure}


%La figura \ref{figura:e_attacks} muestra la evolución de uso de los tipos de ataques. Se muestra obvio que el tipo de ataque más empleado a media que mejoran los bots es el de \emph{attack_Nearest}. Atacar un planeta cercano suele ser una excelente acción, ya que se necesitan menos tiempos para "capturar el planeta" y se da menos tiempo al rival para poder contra-atacar. Es necesario entender que el tiempo que están las "naves" volando de un planeta a otro no están "aportando nada al juego", así que tiene sentido que los mejores bots intenten minizar el tiempo que las naves vuelan de un sitio a otro. Sin embargo acciones como \emph{attack_Weakest} or \emph{attack_Quickest} tienen a ser menos empleadas a medida que mejoran los invidiuos, aunque resultan muy similares a \emph{attack_Nearest}. Se puede extraer que es más beneficioso conquistar un planeta muy cercano (attack_nearest) que uno que sea muy sencillo de conquistar (attack_Quickest) o muy debil (attack_Weakest). %Estrategias como \emph{attack_Wealthiest} y \emph{attack_Beneficial} tienen presencia constante, ya que realizar este tipo de acciones en el momento oportuno supone una gran ventaja para un agente. El \empha{attack_base} atacar la base enemiga sufre un decrecimiento en las primeras generaciones, aunque luego se mantiene constante. De igual manera que en los anteriores casos, el que no desaparezca indica que tiene su utilidad, aunque es necesario encontrar la condición que permite hacer uso de un tipo de ataque en el momento preciso. Por último, destacar que realizar un \emph{attack_Random} tiene una presencia más o menos constante. Aunque sería esperable que este tipo de ataques fuese decreciendo a medida que los individuos mejoran, no sucede así. Puede ser interesante estudiar hasta que punto beneficia a un jugador "hacer algo al azar" frente a realizar una acción más determinista.

Focusing in the {\em type of attacks}, Figure \ref{figura:e_attacks} shows that the \emph{attack\_Nearest} action is the most used one in the Victory fitness, because this action is very beneficial in order to get a fast win, as this fitness function promotes. The reason is that, normally, it takes less time to conquer a close planet than a farther one. The interesting fact is that this type of attack seems to be a better option than the similar \emph{attack\_Weakest} (low number of ships to be conquered) or \emph{attack\_Quickest} (easy to be conquered).

On the contrary, in Slope and Area fitness, attacking the enemy planets with the 
highest growing ratio ({\em wealthiest}) and the ones with high growing ratio but low number of ships ({\em beneficial}) imply less ships for the enemy and more for the player in the following turns, and therefore, a higher fitness value, so they are selected more frequently. 
As could be expected, {\em attacking random planets} do not seem to be a good strategy in any of the methods compared. Attacking the base is not a good strategy neither, even if it was considered as one of the most important actions in previous works that do not use Genetic Programming \cite{Mora_noisy_jcst}.

 \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_ATTACKS_s.pdf}
 \end{center}
 \caption{Evolution of the percentage of every type of attack over the total of attacks used by the best individual by generation of every execution.
 }
 \label{figura:e_attacks}
 \end{figure}

%La figura \ref{figura:e_targets} se presentan los tipos de blancos elegidos al realizar un ataque. Aunque \emph{Target_notMyPlanet} incluye a los otros dos objetivos (\emph{Target_EnemyPlanet} and \emph{Target_NeutralPlanet}, su uso no se incremente a medida que mejoran los individuos. Se puede justificar este comportamiento entendiendo que los planetas neutrales y enemigos requieren mecánicas distintas para ser abordados. El mayor uso de nodos de ataques \emph{Target_EnemyPlanet} en el árbol de decisión de los mejores individuos, muestra que se requieren más reglas para hacer frente al jugador enemigo (que es realmente el oponente y lucha contra nosotros) que para la conquista de planetas neutrales (que no participan activamente en el juego). De igual manera que con el tipo de ataque, el \emph{attack_Random} se mantiene constante.

With respect to the {\em target chosen} in every attack, Victory fitness is again focused in destroy the enemy as quick as possible (\emph{Target\_EnemyPlanet}), avoiding the neutral ones, while the other two methods (Area and Slope) seek any kind of planet to increase their number of ships during the match.

  \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_TARGETS_s.pdf}
 \end{center}
 \caption{Evolution of the percentage of every type of attacking targets over the total of attacks used by the best individual by generation of every execution.
 }
 \label{figura:e_targets}
 \end{figure}

%La figura \ref{figura:e_targetsReinforce} muestra el objetivo de las acciones de tipo re-fuerzo. El refuerzo es un tipo de acción con caracter defensivo u ofensivo en función del objetivo del mimso. Un \emph{reinforce_Wealthiest} puede servir para hacer nuestros planetas más débiles más dificiles de conquistar. Esta estrategia permanece constante aunque no es la prioritaria. Un experto puede justificar que aunque para conseguir la victoria no se deben de dejar descubiertos los planetas más debiles, no es la principal necesidad. El realizar un \emph{reinforce_Base} es menos empleado a media que avanzan las generaciones, aunque hacia la mitad de las generaciones se produce un punto de inflexión que mantiene constante el uso de este tipo de refuerzo. Sin embargo \emph{Reinforce_Nearest} resulta el más empleado por los mejores agentes, llegando a ser cerca del 50% de los tipos de refuerzo realizados. Un experto puede entender que realizar una estrategia que comparta los naves con los planetas más cercanos puede ser muy útil. Cómo se ha comentado anteriormente, mientras las naves se desplazan "de un sitio a otro" no aportan nada al juego, así que una estregia que miniza el tiempo en el que las naves se están desplazando puede resultar más eficiente.

Finally, and concerning the {\em reinforce} action, the three methods create bots that prefer to reinforce nearest planet (\emph{Reinforce\_Nearest}), as shown in Figure \ref{figura:e_targetsReinforce}. This is useful, as travelling ships cannot be used for attack or defence, and minimise this time can be more efficient if used to attack instead reinforce distant planets. However, during the evolution of the Victory fitness a higher oscillation of these percentages can be seen. As previously explained, the bots generated with this method are not focused on the reinforce action, so this number is not maintained during the run. 
It can be seen that \emph{Reinforce\_Base} gets less relevant for the best individuals, since this is less useful if the bot tries to expand its owned planets.

   \begin{figure}[ht]
 \begin{center}
   \includegraphics[width=12cm]{nuevas_imgs/estudio_TARGETS_REINFORCE_s.pdf}
 \end{center}
 \caption{Evolution of the percentage of every type of reinforce over
   the total of reinforces used by the best individual by generation
   of every execution}
% What do you mean by reinforce? Refresh troops? - JJ 
% Antonio - enviar naves a planetas que ya son tuyos, con lo que
% dificultas su conquista
% "reinforce troops" does not exist. Please use real English words. - JJ
 \label{figura:e_targetsReinforce}
 \end{figure}

%Para terminar esta sección, indicar que el realizar un algoritmo de programación genética basado en las reglas decidadas por un jugador experto puede servir también para realimentar el conocimiento del experto sobre el juego. Mediante el estudio de los nodos más empleados por los mejores individuos, nos puede servir para saber que estrategias resultan más "beneficiosas" para conseguir la victoria en el juego.

%Esto puede servir para que el experto diseñe nuevas acciones y condiciones basadas en el conocimiento arrojado por estos mejores individuos. En este caso concreto, se ha descubierto que las acciones que minimizan el tiempo que las naves están volando son más empleadas por los mejores individuos. Diseñar nuevas acciones y condiciones que hagan uso de este criterio puede servir para diversificar y especializar aún más este tipo de reglas. De igual manera, que ninga regla se haya "extinguido" a media que los invidiuos mejoran por el algoritmo genético refleja que el conocimiento del experto que ha sideñado las reglas es bastante bueno, ya que no ha diseñado ninguna relga que resulte useless para conseguir la victoria del juego, como por ejemplo, realizar alguna decisión "estúpida" o que perjudique en lugar de beneficiar al jugador.

%ANTARES: NOTA. Con regla me refiero tanto a accioens como condiciones.


%-----------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%% CONCLUSIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------------------------------------------- 
\section{Conclusions}
\label{sec:conclusion}

The objective of this work is to validate if using Genetic Programming
can create competitive bots for RTS games, and which would be the best fitness function to consider for the evaluation of individuals.
% study behavior or find whichs fitness function is better? - JJ
% Nobody is answering my suggestions? - JJ
% Antonio - respondido
With this aim, three different evaluation functions have been compared in the  generation of bots for the game Planet Wars,namely: Victory-based, Slope-based and Area-based. For everyone, we have tested both, their tolerance to noise and their influence in the quality and structure of the generated bots, i.e. the type of decision tree that the algorithm generates (conditions and actions).
In order to evaluate the quality of the generated agents, a competitive bot
available in the literature (ANONIMOUSBOT) has been considered to fight against it. This bot was the best one obtained in an evolutionary process which optimised different parameters inside a human-designed behavioural engine.  

With respect to the evolutionary process, all the functions seem to work properly, as there is an average improvement in the population along generations, even in the presence of noise. The Victory-based fitness behaves more robustly than the other methods, while the Area-based fitness seems to be less sensitive to noise, and achieves a better percentage of victories in more produced bots.
% Antonio - revisar que esto sea verdad, que ya no estoy pa mucho. :/

The results also show interesting differences in the generated individuals depending on the fitness considered: 
On the one hand, the Victory-based fitness creates more balanced trees in terms of number of nodes and depth, but the actions of the best individuals obtained are more focused in a quick win, performing a more aggressive and fast play. 

On the other hand, as Slope-based and Area-based are methods that take into account the number of ships during the run, they promote to attack the planets with a higher growth rate (being enemies' or not) and reinforce their own planet more frequently than the Victory-based fitness. 

Although the latter function generates bots able to win more times against the bot used for training (ANONIMOUSBOT), when confronting all the generated bots between them, those generated by the Area-based fitness win more times. The reason is that there exists a higher variation in depth and number of nodes in the Victory-based produced bots, 
% Antonio - esto es cierto? Comprobadlo, porfa.
implying more behavioural differences, and probably, that keeping a high number of ships during all the match implies the bot can resist better to the fast and aggressive attacks that Victory-based bots tends to do.


As future lines of work, other rules will be added to the proposed algorithm
(for example, some of them analysing the map) and more competitive enemies
will be used for the evaluation. In addition, the approaches could be implemented and tested in more complex RTS games, such as StarCraft, or even in
different videogames like Unreal\texttrademark~ or Super Mario\texttrademark~. We will also use other ways of dealing with the uncertainty/noises in the evaluation, such as using a Wilcoxon-based method to compare the individuals \cite{merelo2016statistical}. 


%ANTARES: Se me ha ocurrido que como trabajo futuro se podría
%estudiar las "condiciones" y "acciones" de forma junta. Es decir,
%estudiar si una acción concreta tiene como predileción un
%conjunto de condiciones para que cupla concreta. Puede ser costoso de
%computar (no se me ocurre una manera snecilla de hacerlo), pero croe
%que puede ser interesante al menos citarlo como futuro trabajo. 
%cupla? - JJ
% Antonio - será dupla. :D

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  ACKNOWLEDGEMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO: actualizar con los nuevos proyectos
\section*{Acknowledgements}
% This work has been supported in part by projects 
% EPHEMECH (TIN2014-56494-C4-3-P, Spanish Ministerio de Econom? y Competitividad), 
% PROY-PP2015-06 (Plan Propio 2015 UGR), 
% PETRA (SPIP2014-01437, funded by Direcci? General de Tr?ico),
% CEI2015-MP-V17 (awarded by CEI BioTIC Granada), and 
% PRY142/14 (funded by Fundaci? P?blica Andaluza Centro de Estudios
% Andaluces en la IX Convocatoria de Proyectos de Investigaci?).
% Why is this eliminated? 

\bibliographystyle{elsarticle-num}
\bibliography{gpbot-fitness-entcom}
% is it so difficult to use the canonical geneura.bib file? 
% Why do you change the bib file??? - jj
\end{document}
